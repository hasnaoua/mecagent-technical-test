{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom datasets import load_dataset\nimport ast\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom transformers import (\n    LlavaForConditionalGeneration,\n    LlavaProcessor,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    VisionEncoderDecoderModel,\n    TrainingArguments,\n    Trainer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:26:40.989915Z","iopub.execute_input":"2025-06-25T00:26:40.990156Z","iopub.status.idle":"2025-06-25T00:26:52.934749Z","shell.execute_reply.started":"2025-06-25T00:26:40.990130Z","shell.execute_reply":"2025-06-25T00:26:52.934182Z"}},"outputs":[{"name":"stderr","text":"2025-06-25 00:26:48.569398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750811208.595757     275 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750811208.603788     275 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Verify GPU availability\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:26:52.935421Z","iopub.execute_input":"2025-06-25T00:26:52.935988Z","iopub.status.idle":"2025-06-25T00:26:52.940644Z","shell.execute_reply.started":"2025-06-25T00:26:52.935968Z","shell.execute_reply":"2025-06-25T00:26:52.939788Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load dataset with caching\ndataset = load_dataset(\n    \"CADCODER/GenCAD-Code\",\n    num_proc=4,  # Reduced for stability\n    split={\"train\": \"train\", \"test\": \"test\"},\n)\n\n# Inspect dataset structure\nprint(\"Dataset features:\", dataset[\"train\"].features)\nprint(\"Sample:\", dataset[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:26:52.941402Z","iopub.execute_input":"2025-06-25T00:26:52.941658Z","iopub.status.idle":"2025-06-25T00:26:54.448012Z","shell.execute_reply.started":"2025-06-25T00:26:52.941629Z","shell.execute_reply":"2025-06-25T00:26:54.447391Z"}},"outputs":[{"name":"stdout","text":"Dataset features: {'image': Image(mode=None, decode=True, id=None), 'deepcad_id': Value(dtype='string', id=None), 'cadquery': Value(dtype='string', id=None), 'token_count': Value(dtype='int64', id=None), 'prompt': Value(dtype='string', id=None), 'hundred_subset': Value(dtype='bool', id=None)}\nSample: {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=448x448 at 0x798D9A895E10>, 'deepcad_id': '0000/00006371', 'cadquery': 'import cadquery as cq\\n# Generating a workplane for sketch 0\\nwp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.015625, -0.0078125, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\\nloop0=wp_sketch0.moveTo(0.0, 0.0).threePointArc((0.0007948582418457166, -0.0019189575476279677), (0.0027138157894736844, -0.0027138157894736844)).lineTo(0.021217105263157895, -0.0027138157894736844).threePointArc((0.022787161438489866, -0.00206347722796355), (0.0234375, -0.000493421052631579)).lineTo(0.0234375, 0.018256578947368422).threePointArc((0.02283825686147997, 0.019949990385858287), (0.021217105263157895, 0.020723684210526318)).lineTo(0.0022203947368421052, 0.020723684210526318).threePointArc((0.0005992431385200307, 0.019949990385858287), (0.0, 0.018256578947368422)).lineTo(0.0, 0.0).close()\\nsolid0=wp_sketch0.add(loop0).extrude(0.75)\\nsolid=solid0\\n', 'token_count': 1292, 'prompt': 'Generate the CADQuery code needed to create the CAD for the provided image. Just the code, no other words.', 'hundred_subset': False}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"### Model Initialization\nLLAVA_MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\nCEREBRAS_MODEL_ID = \"cerebras/Cerebras-GPT-590M\"\n\n# Load models and components\nprint(\"Loading LLaVA components...\")\nllava_model = LlavaForConditionalGeneration.from_pretrained(\n    LLAVA_MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nllava_processor = LlavaProcessor.from_pretrained(LLAVA_MODEL_ID)\nvision_encoder = llava_model.vision_tower\nvision_encoder.eval()  # Freeze initially\n\nprint(\"\\nLoading Cerebras-GPT components...\")\ndecoder = AutoModelForCausalLM.from_pretrained(\n    CEREBRAS_MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(CEREBRAS_MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token  # Set pad token\n\n\n### Model Configuration\nprint(\"Building VisionEncoderDecoder model...\")\nmodel = VisionEncoderDecoderModel(\n    encoder=vision_encoder,\n    decoder=decoder\n)\n\n# Configure model settings\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# Verify encoder-decoder compatibility\nencoder_dim = model.encoder.config.hidden_size\ndecoder_dim = model.decoder.config.n_embd\nprint(f\"Encoder hidden size: {encoder_dim}\")\nprint(f\"Decoder embedding size: {decoder_dim}\")\n\nif encoder_dim != decoder_dim:\n    print(\"\\n⚠️ Dimension mismatch! Adding projection layer...\")\n    model.encoder.projection = torch.nn.Linear(encoder_dim, decoder_dim)\n    model.config.encoder_hidden_size = decoder_dim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:26:54.450594Z","iopub.execute_input":"2025-06-25T00:26:54.450836Z","iopub.status.idle":"2025-06-25T00:27:51.990650Z","shell.execute_reply.started":"2025-06-25T00:26:54.450817Z","shell.execute_reply":"2025-06-25T00:27:51.989796Z"}},"outputs":[{"name":"stdout","text":"Loading LLaVA components...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b2c7aeeb984f80b51096f36e1ce217"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"\nLoading Cerebras-GPT components...\nBuilding VisionEncoderDecoder model...\nEncoder hidden size: 1024\nDecoder embedding size: 1536\n\n⚠️ Dimension mismatch! Adding projection layer...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"### Data Preprocessing (Updated)\ndef preprocess_function(examples):\n    \"\"\"Process images and text for model input using correct 'cadquery' column\"\"\"\n    images = examples[\"image\"]\n    texts = examples[\"cadquery\"] \n    \n    # Process images\n    image_inputs = llava_processor.image_processor(\n        images,\n        return_tensors=\"pt\"\n    )\n    \n    # Tokenize text\n    text_inputs = tokenizer(\n        texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=256,\n        return_tensors=\"pt\"\n    )\n    \n    # Create labels \n    labels = text_inputs.input_ids.clone()\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    return {\n        \"pixel_values\": image_inputs.pixel_values,\n        \"labels\": labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:31:44.931147Z","iopub.execute_input":"2025-06-25T00:31:44.932065Z","iopub.status.idle":"2025-06-25T00:31:44.938123Z","shell.execute_reply.started":"2025-06-25T00:31:44.931999Z","shell.execute_reply":"2025-06-25T00:31:44.937370Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Apply preprocessing with proper column removal\ncolumns_to_remove = dataset[\"train\"].column_names  # Remove all original columns\n\ntrain_dataset = dataset[\"train\"].select(range(4000)).map(\n    preprocess_function,\n    batched=True,\n    batch_size=4,\n    remove_columns=columns_to_remove  # Remove all original columns\n)\n\neval_dataset = dataset[\"test\"].select(range(100)).map(\n    preprocess_function,\n    batched=True,\n    batch_size=4,\n    remove_columns=columns_to_remove  # Remove all original columns\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:32:47.510408Z","iopub.execute_input":"2025-06-25T00:32:47.511527Z","iopub.status.idle":"2025-06-25T00:32:47.654983Z","shell.execute_reply.started":"2025-06-25T00:32:47.511462Z","shell.execute_reply":"2025-06-25T00:32:47.654173Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"### Training Configuration\ntraining_args = TrainingArguments(\n    output_dir=\"./genCAD-vision-coder\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    save_strategy=\"epoch\",\n    num_train_epochs=3,\n    learning_rate=5e-5,\n    fp16=True,\n    logging_steps=50,\n    report_to=\"none\",\n    save_total_limit=2,\n    dataloader_num_workers=4,\n)\n\n### Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:32:52.627942Z","iopub.execute_input":"2025-06-25T00:32:52.628590Z","iopub.status.idle":"2025-06-25T00:32:52.679430Z","shell.execute_reply.started":"2025-06-25T00:32:52.628564Z","shell.execute_reply":"2025-06-25T00:32:52.678608Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"### Start Training\nprint(\"Starting training...\")\ntrain_results = trainer.train()\nprint(\"Training completed!\")\n\n# %% [code]\n### Save Final Model\nmodel.save_pretrained(\"./genCAD-vision-coder-final\")\ntokenizer.save_pretrained(\"./genCAD-vision-coder-final\")\nprint(\"Model saved successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:31:50.784088Z","iopub.execute_input":"2025-06-25T00:31:50.784375Z","iopub.status.idle":"2025-06-25T00:31:58.950405Z","shell.execute_reply.started":"2025-06-25T00:31:50.784352Z","shell.execute_reply":"2025-06-25T00:31:58.947562Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_275/4106158500.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Start Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py\", line 606, in forward\n    encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: mat1 and mat2 must have the same dtype, but got Half and Float\n"],"ename":"RuntimeError","evalue":"Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py\", line 606, in forward\n    encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n    return F.linear(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: mat1 and mat2 must have the same dtype, but got Half and Float\n","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"### Run Evaluation on Test Subset\nmodel.eval()  # Set model to evaluation mode\ndevice = model.device\n\n# Select a small subset for evaluation\nsubset_size = 10\neval_subset = dataset[\"test\"].select(range(subset_size))\n\n# Store results\nsyntax_results = []\niou_results = []\n\nfor example in eval_subset:\n    # Preprocess image\n    pixel_values = llava_processor.image_processor(\n        example[\"image\"],\n        return_tensors=\"pt\"\n    ).pixel_values.to(device)\n    \n    # Generate CAD code\n    outputs = model.generate(\n        pixel_values=pixel_values,\n        max_length=256,\n        num_beams=5,\n        early_stopping=True\n    )\n    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # 1. Calculate Syntax Validity\n    is_valid = valid_syntax_rate_simple(generated_code)\n    syntax_results.append(is_valid)\n    \n    # 2. Calculate IoU\n    try:\n        # Render generated CAD code\n        generated_image = render_cad_code(generated_code)\n        \n        # Get ground truth image\n        gt_image = example[\"image\"]\n        \n        # Calculate IoU\n        iou = get_iou_best(gt_image, generated_image)\n        iou_results.append(iou)\n        \n        # Visualization (optional)\n        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n        axes[0].imshow(gt_image)\n        axes[0].set_title(\"Ground Truth\")\n        axes[0].axis('off')\n        \n        axes[1].imshow(generated_image)\n        axes[1].set_title(f\"Generated (IoU: {iou:.2f})\")\n        axes[1].axis('off')\n        \n        plt.suptitle(f\"Syntax: {'Valid' if is_valid else 'Invalid'}\")\n        plt.show()\n        \n    except Exception as e:\n        print(f\"Rendering failed: {str(e)}\")\n        iou_results.append(0.0)\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:28:03.282793Z","iopub.status.idle":"2025-06-25T00:28:03.283114Z","shell.execute_reply.started":"2025-06-25T00:28:03.282950Z","shell.execute_reply":"2025-06-25T00:28:03.282970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Calculate Final Metrics\n# 1. Syntax Validity Rate\nvalid_syntax_rate = np.mean(syntax_results)\nprint(f\"\\nSyntax Validity Rate: {valid_syntax_rate:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:28:03.283638Z","iopub.status.idle":"2025-06-25T00:28:03.283948Z","shell.execute_reply.started":"2025-06-25T00:28:03.283796Z","shell.execute_reply":"2025-06-25T00:28:03.283810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Average IoU\naverage_iou = np.mean(iou_results)\nprint(f\"Average IoU: {average_iou:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:28:03.288287Z","iopub.status.idle":"2025-06-25T00:28:03.288848Z","shell.execute_reply.started":"2025-06-25T00:28:03.288654Z","shell.execute_reply":"2025-06-25T00:28:03.288671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Combined Metric\ncomposite_score = 0.7 * valid_syntax_rate + 0.3 * average_iou\nprint(f\"Composite Score: {composite_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T00:28:03.290232Z","iopub.status.idle":"2025-06-25T00:28:03.292003Z","shell.execute_reply.started":"2025-06-25T00:28:03.291825Z","shell.execute_reply":"2025-06-25T00:28:03.291843Z"}},"outputs":[],"execution_count":null}]}